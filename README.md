# ALEXSIS2.0

The ALEXIS+ and ALEXSIS++ datasets are divided into three sub-corpora corresponding to English (EN), Spanish (ES), and Portuguese (PT). Each dataset has the following nine headers separated by tab (\textbackslash t):  

<ul>
  <li><b>ID</b>: instance id that is made up of the original instance id (e.g. 01) and the new additional context id. (e.g. 104): 01-104.</li>
  <li><b>ALEXSIS.CW</b>: the original complex word taken form ALEXSIS and used at TSAR-2022.</li>
  <li><b>ALEXSIS.Context</b>: the original context for the given complex word taken form ALEXSIS and used at TSAR-2022.</li>
  <li><b>Candidate.Subs@n</b>: the candidate substitutions generated using MLM on the instances provided by TSAR-2022.</li>
  <li><b>Additional.Context</b>: new additional context obtained from the CC-News dataset.</li>
  <li><b>Additional.Subs@n</b>: new additional candidate substitutions generated using MLM on the additional contexts taken from the CC-News dataset.</li>
  <li><b>Sent.Sim</b>: the cosine similarities between the SBert sentence embedding of the additional context and the original context provided by TSAR-2022. </li>
  <li><b>Word.Sim</b>: : the cosine similarities between the word embeddings of the additional candidate substitutions and the original complex word provided by TSAR-2022.</li>
  <li><b>Gold.Labels</b>: the original gold candidate substitutions provided by TSAR-2022.  </li>
</ul>


ALEXSIS+ has a total of 12,831, 13,353, and 13,541 matched complex words in unique contexts for English, Spanish, and Portuguese, respectively. The larger ALEXSIS++ dataset contains matched complex words in 33,149 unique contexts only for English. 

Both datasets provide embedding similarity scores between their additional sentences and the original context (Sent.Sim), as well as between their additional candidate substitutions and the original complex word (Word.Sim). Sentence embeddings were generated using Sentence-BERT (SBert)\footnote{SBert: \url{https://huggingface.co/sentence-transformers/stsb-mpnet-base-v2}} \cite{reimers-gurevych-2019-sentence}. 

SBert is a state-of-the-art sentence-transformer. It is a modification of the pre-trained LLM BERT that employs siamese and triplet network structures to produce sentence embeddings that can be used to compare the cosine-similarity between sentences. 

Word similarity scores were generated by calculating the cosine similarity between word embeddings. English word embeddings were obtained using the \textit{en-vectors-web-lg}\footnote{en\_vectors\_web\_lg: \url{https://huggingface.co/spacy/en_core_web_lg}} model that provides approximately ~500k word vector representations. Spanish and Portuguese word embeddings were taken from the \textit{pt-core-news-lg}\footnote{pt\_core\_news\_lg: \url{https://huggingface.co/spacy/pt_core_news_lg}} and \textit{es-core-news-lg }\footnote{es\_core\_news\_lg: \url{https://huggingface.co/spacy/es_core_news_lg}} models trains on crawled news articles. 
